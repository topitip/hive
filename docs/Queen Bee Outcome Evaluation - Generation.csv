Question,Entry Point,Request,openai/gpt-5-mini,gemini/gemini-3-flash-preview,anthropic/claude-haiku-4-5,zai/glm-5,Desirable Response,,Optimization Method
What can you do?,Deep Research Agent,,"I can do four main things:
Build agents end-to-end
Design graph (2–4 nodes), prompts, tools, edges
Implement files under exports/{agent_name}
Validate with:
default_agent.validate()
AgentRunner.load(...)
validate_agent_tools(...)
run_agent_tests(...)
Load the built agent into this session immediately
Modify/fix existing agents
Debug load failures, unreachable nodes, bad exports, tool mismatches
Inspect sessions/checkpoints/memory for failing runs
Patch code and re-test until passing
Do general coding tasks directly
Read/edit/write files
Run shell commands/tests
Refactor, add features, fix bugs
Run the research worker when appropriate
It can perform interactive web research with citations and produce reports
I can start it, monitor progress, pass your instructions, and stop it if needed
If you want, give me one concrete task now (build, fix, or research), and I’ll start.",,"What I Can Do
Agent Building & Development
Design agents — Understand your use case, propose architecture, get approval
Implement agents — Write all files (config, nodes, graph, tests) following framework conventions
Validate & test — Run structural validation, tool checks, and pytest suites
Debug agents — Inspect session state, memory, checkpoints, execution logs
Read & modify code — Consult reference agents, fix bugs, refactor
Code & File Work
Read/write/edit files with fuzzy matching
Search codebases with regex
Run shell commands (Python, tests, builds)
Undo changes via git
Agent Lifecycle
List agents — See all built agents and their session counts
Monitor sessions — Check status, memory, checkpoints of running agents
Load agents — Make a built agent available as the worker in this session
Run tests — Execute pytest on agent test suites with structured results
Worker Delegation
Start the worker — Give it a research task; it runs autonomously
Monitor progress — Check status, health, recent steps
Send messages — Inject instructions while it's running
Stop execution — Cancel the worker's current task
Critical Rules I Follow
Read before writing — Never assume code structure; read reference agents first
Verify tools — Always discover available tools dynamically; never guess
Test after building — Validate structure, load, tools, then run tests
Professional defaults — Use framework patterns exactly; no shortcuts
2-4 nodes MAX — Merge thin nodes aggressively to preserve context
What's your next move?","Direct capabilities (I do these myself):

Category        What I can do
File operations        Read, write, edit, search, list files anywhere in the project
Shell commands        Run any command (tests, scripts, git, etc.)
Agent building        Design and implement Hive agents from scratch
Validation        Test agent structure, verify tools exist, run pytest
Debugging        Inspect sessions, checkpoints, memory for built agents
Code changes        Any coding task — bug fixes, features, refactoring
Delegation capability:

The deep research worker can search the web, scrape sources, and produce cited reports. I delegate to it when you need research on a topic.

Typical workflows:

""Build me an agent that..."" → I design (2-4 nodes), implement, validate, and load it into this session
""Debug my failing agent"" → I inspect sessions, checkpoints, memory to find the issue
""Research X topic"" → I start the worker to search and compile findings
""Change this code"" → I read, edit, and test directly
Limits:

I don't fabricate tools — I verify what exists first
I follow existing patterns, not assumptions
I validate before declaring success
What do you need?","Direct capabilities (I do these myself):

Category        What I can do
File operations        Read, write, edit, search, list files anywhere in the project
Shell commands        Run any command (tests, scripts, git, etc.)
Agent building        Design and implement Hive agents from scratch
Validation        Test agent structure, verify tools exist, run pytest
Debugging        Inspect sessions, checkpoints, memory for built agents
Code changes        Any coding task — bug fixes, features, refactoring
Delegation capability:

The deep research worker can search the web, scrape sources, and produce cited reports. I delegate to it when you need research on a topic.

Typical workflows:

""Build me an agent that..."" → I design (2-4 nodes), implement, validate, and load it into this session
""Debug my failing agent"" → I inspect sessions, checkpoints, memory to find the issue
""Research X topic"" → I start the worker to search and compile findings
""Change this code"" → I read, edit, and test directly
Limits:

I don't fabricate tools — I verify what exists first
I follow existing patterns, not assumptions
I validate before declaring success
What do you need?",,System Prompt
What can you do?,Passive Vulnerability Assessment,,,"I have two primary roles: as your primary coding interface and as the manager of the Hive framework.

1. Direct Coding & Agent Building
I can handle any technical task involving the framework or general software development:

Build New Agents: I can design, implement, and verify custom agents in the exports/ directory.
Modify & Debug: I can read existing code, fix bugs, or expand the capabilities of your current agents.
Framework Management: I can discover tools via MCP, run validation tests, and manage agent sessions.
Direct File Operations: I can read, write, and edit files across the project.
2. Specialized Security Worker
I currently have a Vulnerability Assessment worker loaded and ready. This worker is specialized for:

Passive Reconnaissance: Non-intrusive scanning of websites (SSL, DNS, Headers, Ports, Tech Stack).
Risk Scoring: Generating A-F letter grades for different security categories.
Remediation Reporting: Providing actionable security guidance for developers.
How would you like to proceed? I can start a security scan for you, or we can work on building/modifying something in the codebase.","I'm a coding agent with deep Hive framework integration. Here's what I can do:

File I/O
Read/write/edit files with line numbers and fuzzy matching
Search files with regex
Run shell commands with PYTHONPATH pre-configured
Undo changes via git snapshots
Agent Building (Full Lifecycle)
Understand & design — Listen to your requirements, surface problems early, play back a concrete architecture before I build
Implement — Write agent code following framework conventions (nodes, edges, system prompts, tool integration)
Verify — Run four validation steps (class validation, runner load test, tool validation, pytest) and fix errors myself
Load into session — Make the agent immediately available to interact with (no manual CLI needed)
Agent Debugging
List agents and their sessions
Inspect session state, memory, checkpoints
Run test suites with structured result parsing
Trace execution flow and diagnose failures
Tool Discovery
List all available tools grouped by category
Discover MCP tool schemas with full parameter details
Validate that tools declared in agents actually exist
Worker Lifecycle Management
Start the worker with a task description (delegates to the vulnerability_assessment-graph agent)
Stop the worker or inject messages mid-execution
Check worker status (idle, running, waiting for input)
Monitor worker health and read execution logs
Escalate critical issues to you with analysis and recommended action
Code Quality
Enforce framework conventions (imports, structure, patterns)
Catch hallucinated tool names before runtime
Validate graph structure, node specs, edges
Self-verify all work before declaring success
I do NOT do: Guess or assume. I read reference code, run discovery commands, and verify every assumption before writing. I surface deal-breakers early. I measure success with validation, not optimism.

What would you like to build or fix?",,,,Tool Use? 
What are you?,Job Hunter ,,"I’m the Queen: your primary coding and agent-building interface in this Hive session.

I can:

Read/edit code and run commands
Build/modify/debug Hive agents end-to-end
Validate and test agents
Manage a background worker specialized for deep web research with citations","I am the Queen—your primary interface for the Hive framework. Technically, I am a coding agent with full access to this project's source code, tools, and the underlying framework infrastructure.

Think of me as your Architect and Lead Developer. My core capabilities include:

Agent Engineering: I can design and build specialized autonomous agents from scratch, following strict framework conventions.
System Orchestration: I manage the lifecycle of ""Worker"" agents. I can delegate specific tasks to them (like the Vulnerability Assessment worker currently available) while I continue to work on other things.
Codebase Mastery: I have direct access to read, write, and execute code within this environment. I can debug existing agents, run test suites, and discover new tools dynamically.
Quality Control: I verify everything I build using structural validation and automated tests before declaring a task complete.
In short: I am the central intelligence that builds and manages the autonomous ecosystem you see here. If you need a new capability, I build it. If you have a specific task, I can either do it myself or deploy a worker to handle it.

What would you like to build or explore first?","I'm the Queen — your primary interface to Hive.

I'm a coding agent with full file I/O, shell execution, and agent-building capabilities. I can read code, write code, run tests, validate agents, and manage the framework.

I also manage the worker — the job_hunter-graph agent that's currently idle. When you have a task that matches the worker's goal (resume analysis, job search, customization), I delegate to it via start_worker(). For everything else — building agents, debugging code, modifications — I do it directly.

You have two agents available:

Me (Queen) — Handles all coding, building, configuration, debugging
Worker (job_hunter-graph) — Handles job hunting workflows
How can I help?","I'm the Queen — a coding agent and your primary interface in the Hive system.

My role:

Primary interface between you and the Hive framework
Build, modify, and debug agents directly
Manage the research worker's lifecycle
Read/write files, run commands, validate and test code
The worker (deep_research_agent) is a specialized subordinate that runs autonomously when delegated research tasks. I can start, stop, monitor, and relay messages to it.

Hierarchy:

You → give instructions
Me → execute directly or delegate to worker
Worker → runs research tasks autonomously
I'm not a chatbot. I'm a coding agent that can actually read your codebase, write files, and build working agents.","I'm the Queen — a coding agent and your primary interface in the Hive system.

My role:

Primary interface between you and the Hive framework
Build, modify, and debug agents directly
Manage the research worker's lifecycle
Read/write files, run commands, validate and test code
The worker (deep_research_agent) is a specialized subordinate that runs autonomously when delegated research tasks. I can start, stop, monitor, and relay messages to it.

Hierarchy:

You → give instructions
Me → execute directly or delegate to worker
Worker → runs research tasks autonomously
I'm not a chatbot. I'm a coding agent that can actually read your codebase, write files, and build working agents.",,Train Model? 
Run this agent,Deep Research Agent,,"Started. I told the worker to run now, proceed through intake, and ask you for any missing details needed to generate the report.",,"Available agents (user-built):

competitive_intel_agent_graph — Competitive Intelligence Digest
inbox_management — Gmail inbox management (3 sessions)
template_agent — Generic template
vulnerability_assessment — Passive security scanning (7 sessions)
Which one? Or did you mean:

Load one as the worker (so you can interact with it) — load_built_agent(""exports/{name}"")
Start it with a specific task — start_worker(""task description"")
Run tests on it — run_agent_tests(""{name}"")
Specify the agent name and what you want to do. Escalation: Stalled Worker Session
Severity: CRITICAL

Issue: Worker session session_20260209_202854_784c9535 (deep_research_agent) has been cancelled and stalled for 16.7 days. Last activity: 2026-02-10 04:30 UTC. Status explicitly marked as cancelled. No new iterations since initialization.

Root cause: The session was explicitly cancelled — likely a previous task that was abandoned or interrupted.

Recommended action: This is not a system failure requiring restart. The session is legitimately terminated. Mark as resolved — no operator intervention needed unless you want to start a fresh research task.

What would you like to do?

Start a fresh worker session — start_worker(""your research topic"")
Inspect the old session — Read its memory/checkpoints to see what happened
Ignore — Session is dead; move forward with new work
Which?",The research agent is ready to run. What topic would you like it to research?,The research agent is ready to run. What topic would you like it to research?,,
Run this agent,Passive Vulnerability Assessment,,,"I have a Vulnerability Assessment worker ready to run. To start it, I just need the domain name of the website you want to assess.

Please provide the website URL or domain (e.g., example.com).

Once you provide the target, I will:

Initialize the worker with your request.
Monitor its progress as it performs passive reconnaissance (SSL, DNS, Tech stack, etc.).
Notify you when it has findings ready for your review.","Queen Bee
Queen
Starting the worker with a task. I need the target website domain.

What domain should the vulnerability assessment scan?",,,,Remove list template tools
Run this template,,,"Which template do you want me to run?

Share one of:

the agent name/path (e.g. exports/some_agent), or
the template file path, or
paste the template content here.
If you mean the deep research worker template, say the research topic and I’ll start it now.","I have access to over 200 tools across categories like Gmail, GitHub, Cal.com, Apollo, and Web Search (Exa).

To ""run this template,"" I need to know:

Which template are you referring to? (e.g., a Research agent, a Personal Assistant, a Gmail manager?)
What should it do? (e.g., ""Summarize my unread emails and add them to a sheet."")
If you give me a brief description, I will design the graph (usually 2-4 nodes), show you the plan, and then implement it for you. What do you have in mind?","I need clarification. Which template would you like me to run?

Agent template — I can generate a complete agent package (config, nodes, graph, tests) in exports/ with a name you choose
Specific reference agent — I can show you an existing agent's structure (e.g., deep_research_agent)
File template — I can create individual files (nodes/init.py, agent.py, etc.)
What's your use case? Describe briefly what the agent should do, and I'll scaffold it for you.",Worker just started. Let me check progress in a moment.,Worker just started. Let me check progress in a moment.,,Remove list template tools
Execute,Deep Research Agent,,"Ready to execute — what should I run?

If you mean the Gmail inbox worker, send:

rules: plain-English actions (e.g., “newsletters → mark read + archive; invoices → star + label Finance”)
max_emails (optional, default 100)",,"I need more information. What should the worker execute?

For the vulnerability assessment agent, provide:

The target website domain (e.g., example.com)
Any specific scan preferences or scope notes
Example: start_worker(""Scan example.com for vulnerabilities using passive OSINT techniques"")

What domain should I scan?","I need more context. What would you like me to execute?

Start the worker on a research task? Tell me the topic.
Build a new agent? Describe what it should do.
Run a command or script? Give me the command.
Something else? Just describe it.","I need more context. What would you like me to execute?

Start the worker on a research task? Tell me the topic.
Build a new agent? Describe what it should do.
Run a command or script? Give me the command.
Something else? Just describe it.",,Remove list template tools
Execute,Passive Vulnerability Assessment,,,"I'm ready to execute, but I need a specific instruction.

If you want to run the existing worker, please provide a domain name:

Example: ""Run the vulnerability scanner on google.com""
If you want me to build and execute a new agent, tell me what it should do:

Example: ""Build an agent that searches for news about AI and saves it to a file.""
What would you like me to start?",,,,,
Hello/Hi,,,"Hello. What would you like to build or fix?/Hi. Share the task, and I’ll handle it.","Hello! I am the Queen. I am ready to help you build or manage agents in the Hive framework.

I currently have a Vulnerability Assessment worker available. If you would like to run a security scan, just provide a domain (e.g., example.com).

Otherwise, tell me what you would like to build or modify, and I'll get to work. What's on your mind?",,Hello. What can I help you with today?,Hello. What can I help you with today?,,
How does this work?,Deep Research Agent,,"You describe a task, and I route it to the right mode:

Research task (topic investigation, sources, synthesis): I start the deep research worker.

It runs: intake → research → review → report
It can search/scrape web sources, save artifacts, and return a cited report.
Engineering task (build agent, edit code, fix tests, debug runtime): I do it directly here.

I read relevant files first, implement changes, then run validation/tests before declaring done.
If you want, give me one concrete task now (research or coding), and I’ll start immediately.",,,"The Hive Framework

This is a system for building and running AI agents. You're talking to me (the Queen), which manages a worker agent and can also do tasks directly.

Worker Agent (deep_research_agent)

The worker runs autonomously on research tasks. Its workflow:

┌─────────┐     ┌──────────┐     ┌────────┐     ┌────────┐
│ intake  │ ──► │ research │ ──► │ review │ ──► │ report │
│ (talk)  │     │ (search) │     │ (talk) │     │ (write)│
└─────────┘     └──────────┘     └────────┘     └────────┘
     ▲                               │              │
     └───────────────────────────────┘              │
                    (revise)                        ▼
                                              [HTML report]
intake: You describe what to research, it clarifies scope
research: Searches web, fetches sources, compiles findings
review: Shows you findings, asks if it should dig deeper or write
report: Produces a cited HTML report
Me (The Queen)

I can:

Start the worker: start_worker(""research quantum computing applications"")
Monitor progress: Check status, health, inject messages
Build agents: Create new agents from scratch
Direct coding: Read files, write code, run commands, debug
How to use this

Research task: Just say ""research [topic]"" — I'll start the worker
Build an agent: Describe a workflow (e.g., ""agent that monitors my email and drafts responses"")
Code task: Ask me to read/write files or run commands
What would you like to do?","The Hive Framework

This is a system for building and running AI agents. You're talking to me (the Queen), which manages a worker agent and can also do tasks directly.

Worker Agent (deep_research_agent)

The worker runs autonomously on research tasks. Its workflow:

┌─────────┐     ┌──────────┐     ┌────────┐     ┌────────┐
│ intake  │ ──► │ research │ ──► │ review │ ──► │ report │
│ (talk)  │     │ (search) │     │ (talk) │     │ (write)│
└─────────┘     └──────────┘     └────────┘     └────────┘
     ▲                               │              │
     └───────────────────────────────┘              │
                    (revise)                        ▼
                                              [HTML report]
intake: You describe what to research, it clarifies scope
research: Searches web, fetches sources, compiles findings
review: Shows you findings, asks if it should dig deeper or write
report: Produces a cited HTML report
Me (The Queen)

I can:

Start the worker: start_worker(""research quantum computing applications"")
Monitor progress: Check status, health, inject messages
Build agents: Create new agents from scratch
Direct coding: Read files, write code, run commands, debug
How to use this

Research task: Just say ""research [topic]"" — I'll start the worker
Build an agent: Describe a workflow (e.g., ""agent that monitors my email and drafts responses"")
Code task: Ask me to read/write files or run commands
What would you like to do?",,
How does this work?,Passive Vulnerability Assessment,,,"This system works through a Queen-Worker relationship, where I (the Queen) manage the environment and specialized agents (the Workers) handle the execution.

Here is the breakdown of the three main layers:

1. The Queen (Me)
I am your primary interface and a Meta-Agent. I don't just chat; I have deep access to the codebase. I can:

Design & Build: I can create new agents in the exports/ folder from scratch.
Monitor: I can see what workers are doing, read their logs, and check their ""health"" via a judge system.
Modify: I can edit any file in the project to fix bugs or add features.
Discover: I dynamically find tools available in the environment so I never guess what's possible.
2. The Workers
Workers are specialized agents built for specific tasks.

Right now, I have a Vulnerability Assessment worker loaded.
When you give it a task (e.g., ""Scan example.com""), it runs autonomously through a defined graph of nodes.
It uses specific tools (SSL scanners, DNS tools, tech detectors) to achieve its goal.
3. The Graph (The ""Brain"")
Every agent is defined by a Directed Acyclic Graph (DAG):

Nodes: Individual ""rooms"" where an LLM performs a specific task (like ""Recon"" or ""Reporting"") using a set of tools.
Edges: The paths between nodes. For example: If Scan Success → Go to Scoring; If Scan Failure → Go to Retry.
Shared Memory: As an agent moves between nodes, it saves important data to a shared memory space so the next node knows what happened.
How to use it:
To Run a Task: Tell me to start the worker on a specific target.
To Build Something: Tell me a use case (e.g., ""I want an agent that manages my GitHub issues""). I will design it, show you the map, and write the code.
To Debug: If an agent is acting up, ask me to ""check the status"" or ""debug the session,"" and I'll dive into the logs.
Would you like to try running the vulnerability worker, or should we build a new agent together?",,,,,
Can you add [services / nodes],Deep Research Agent,,"Yes. I can add that.

Before I implement: should this be for the existing deep_research_agent flow (after report generation), and should it:

ask the user for recipient + subject each time, or
use a saved default recipient? 

(Failed to do so, and started the worker instead)",,,"Let me first examine the current deep_research_agent structure.

I found the deep_research_agent template. It currently has 4 nodes: intake → research → review → report (loop back).

Current flow:

intake → research → review → report → (back to intake)
Proposed change:
Add an email_report node after report that sends the HTML report via email:

intake → research → review → report → email_report → (back to intake)
Design questions:

Which email service? Available options:

gmail_send_email (if you use Gmail)
brevo_send_email (Brevo/Sendinblue)
Where to collect your email address?

Option A: Add to intake — ask for email at the start with the research topic
Option B: Ask in email_report node — prompt before sending each report
Where to modify?

The template is at examples/templates/deep_research_agent/
Should I modify the template directly, or create a new agent in exports/?
Which email service do you prefer, and should I collect the email address upfront (intake) or at send-time (email node)?","Let me first examine the current deep_research_agent structure.

I found the deep_research_agent template. It currently has 4 nodes: intake → research → review → report (loop back).

Current flow:

intake → research → review → report → (back to intake)
Proposed change:
Add an email_report node after report that sends the HTML report via email:

intake → research → review → report → email_report → (back to intake)
Design questions:

Which email service? Available options:

gmail_send_email (if you use Gmail)
brevo_send_email (Brevo/Sendinblue)
Where to collect your email address?

Option A: Add to intake — ask for email at the start with the research topic
Option B: Ask in email_report node — prompt before sending each report
Where to modify?

The template is at examples/templates/deep_research_agent/
Should I modify the template directly, or create a new agent in exports/?
Which email service do you prefer, and should I collect the email address upfront (intake) or at send-time (email node)?",,
Can you add [services / nodes],Passive Vulnerability Assessment,,,"Yes, I can add a new service to the framework. Depending on what you need, this usually means one of two things:

Adding a new Worker Agent: Building a complete autonomous agent (like the Vulnerability scanner) to handle a specific workflow.
Adding new Tools (MCP): Integrating a new API or capability so that any agent can use it.
To get started, tell me about the service you have in mind:

What is the goal? (e.g., ""I want to monitor my GitHub repository for new stars."")
What tools/APIs does it need? (e.g., ""It needs to check GitHub and send me a Slack message."")
Is it interactive? (Should it ask you questions, or just run in the background?)
Once you provide the details, I will:

Check available tools to see if we already have the necessary integrations.
Design the graph (nodes and edges).
Implement the code in a new exports/ package.
What service should we build?",,,,,
Can you do [an action],Deep Research Agent,Can you send me an email to me at timothy@adenhq.com on how this agent works,"I can’t send email directly from here with the current worker/tool setup.

Fastest path: I can generate the exact email draft now (subject + body) that you can paste/send, or I can build you a small mailer agent that can actually send it if your MCP tools include an email provider.",,,"I can send you an email using the send_email tool. I need two things:

Your email address — where should I send it?
Provider preference — do you have Gmail or Resend configured? (Gmail is connected via hive.adenhq.com, Resend needs an API key)
The email will explain how the deep research agent works: its 4-stage pipeline (intake → research → review → report), how it searches diverse sources, scrapes content, and produces cited HTML reports with user checkpoints.","I can send you an email using the send_email tool. I need two things:

Your email address — where should I send it?
Provider preference — do you have Gmail or Resend configured? (Gmail is connected via hive.adenhq.com, Resend needs an API key)
The email will explain how the deep research agent works: its 4-stage pipeline (intake → research → review → report), how it searches diverse sources, scrapes content, and produces cited HTML reports with user checkpoints.",,
Can you do [an action],Passive Vulnerability Assessment,,,"I can perform the reconnaissance directly. Since I have access to the same tools as the worker, I can execute the scan, analyze the results, and provide you with a summary right here in our conversation.

I'll use the vulnerability assessment tools (SSL/TLS, HTTP Headers, DNS, Port Scan, Tech Stack, and Subdomains) to perform a passive, non-intrusive scan.

Which website domain should I analyze? (e.g., example.com)

",,,,,
Can you change whatever existing [node],Deep Research Agent,Change the report node's criteria so it creates markdown document instead of html,"Here's how I plan to change the report node:

Update the node's brief/description/comment that currently says """"HTML"""" → """"Markdown"""".
Ensure the success_criteria explicitly requires a Markdown report (it already references Markdown, I'll verify consistency).
Edit the system_prompt examples and tool-call snippets to show Markdown (headings, lists, citation notation like [n]) instead of HTML tags (e.g., replace <h2>... with """"## ..."""", replace <div>/<h3> blocks with Markdown equivalents).
Keep the file name (""""report.md""""), YAML frontmatter, and the requirement to build the file via save_data + append_data calls.
Preserve the instruction that append_data must be used for sections to avoid token limits.
One quick question before I modify files:

Keep the filename report.md and the existing citation style ([n]) as-is? (yes/no)
If you confirm, I'll edit exports/deep_research_agent/nodes/init.py and run validations.""",,,"Let me find the report generation logic in the deep research agent.

(long wait)

I found the report node in examples/templates/deep_research_agent/nodes/__init__.py. It currently writes HTML with multi-step append_data calls and embedded CSS. I'll change it to produce markdown instead.

Done. The report node in examples/templates/deep_research_agent/nodes/__init__.py now produces markdown instead of HTML:

Changes made:

Description: ""Write a cited HTML report"" → ""Write a cited markdown report""
Success criteria: ""HTML report"" → ""markdown report""
System prompt: Simplified from 7-step HTML/CSS process to 4-step markdown process
Output filename: report.html → report.md
Removed embedded CSS and HTML div structure
Added markdown template with standard headers, links, and lists
The worker agent will now generate markdown reports with proper formatting when it reaches the report node.","Let me find the report generation logic in the deep research agent.

(long wait)

I found the report node in examples/templates/deep_research_agent/nodes/__init__.py. It currently writes HTML with multi-step append_data calls and embedded CSS. I'll change it to produce markdown instead.

Done. The report node in examples/templates/deep_research_agent/nodes/__init__.py now produces markdown instead of HTML:

Changes made:

Description: ""Write a cited HTML report"" → ""Write a cited markdown report""
Success criteria: ""HTML report"" → ""markdown report""
System prompt: Simplified from 7-step HTML/CSS process to 4-step markdown process
Output filename: report.html → report.md
Removed embedded CSS and HTML div structure
Added markdown template with standard headers, links, and lists
The worker agent will now generate markdown reports with proper formatting when it reaches the report node.",,"Tell Queen the loaded agent's file path, otherwise agent will run for a long time"
Can you change whatever existing [node],Passive Vulnerability Assessment,,,"I can certainly help with that. To ensure I make the right changes, I first need to see the current implementation of the passive-recon node.

I'll start by listing the agents to confirm the exact package name and location, then I'll read the node definitions.

",,,,,
I don’t have API credentials. Can you run without APIs / credentials ,Deep Research Agent,,"Yes — you can still proceed, but with limitations. Options:

Provide API credentials (best): agent will run fully — LLM-driven search, analysis, and a real cited report.

Mock/demo mode (no credentials): I can start the worker in mock mode to demonstrate the flow and produce placeholder outputs (structure, simulated summaries, saved files). This will NOT perform real LLM reasoning or authoritative web research — outputs are synthetic for demo and testing only.

Structural run / validation only: I can run all agent validation steps (graph, node/tool checks, tests) and show exactly what would run without actually executing LLM-driven tasks.

Which would you like me to do now? If mock/demo, give a one-line task (e.g., ""Demo: research solid-state batteries"") or say ""structural validation"" to only validate and report.",,,"Yes and no.

Works without API credentials:

Building agents (file creation, structure)
Validating agents (graph structure, tool checks, AgentRunner.load())
Structural tests
Code exploration and modification
Needs API credentials:

Running agents interactively (TUI)
Worker tasks (research, tool calls)
Any actual LLM execution
What you can do right now:

Design and build new agents — I can create the full package
Validate structure — all checks pass without keys
Modify existing agents — code changes work fine
The agent will be ready to run once you add credentials. Would you like to build something?","Yes and no.

Works without API credentials:

Building agents (file creation, structure)
Validating agents (graph structure, tool checks, AgentRunner.load())
Structural tests
Code exploration and modification
Needs API credentials:

Running agents interactively (TUI)
Worker tasks (research, tool calls)
Any actual LLM execution
What you can do right now:

Design and build new agents — I can create the full package
Validate structure — all checks pass without keys
Modify existing agents — code changes work fine
The agent will be ready to run once you add credentials. Would you like to build something?",,
I don’t have API credentials. Can you run without APIs / credentials ,Passive Vulnerability Assessment,,,"I can certainly work on code directly without API keys (reading, writing, and architecting agents). However, executing agents or the worker requires LLM access and tool credentials.

Here is what we can do without API keys:

Agent Design & Implementation: I can build full agent packages (config, nodes, graph, tests) in the exports/ directory.
Structural Validation: I can run default_agent.validate() and AgentRunner.load() tests to ensure the Python code and graph logic are perfectly sound.
Mock Testing: I can write and run tests using ""mock mode"" to verify the flow of the agent without actually calling an LLM.
Note on the Worker: I cannot start the competitive_intel_agent worker for you right now because it requires an active LLM provider to function.

Would you like to build a new agent together, or shall I walk you through how to configure your credentials in ~/.hive/configuration.json when you're ready?

",,,,,
Delete the node (intake),Deep Research Agent,delete the node report,"Here's how I understand the request and the safe choices going forward.

You asked: ""delete the node report"".

What that entails technically

Remove the report NodeSpec from exports/deep_research_agent/nodes/init.py.
Remove it from the nodes list and any imports in exports/deep_research_agent/agent.py.
Remove edges that reference ""report"" (e.g., review→report and report→email).
Update exports/deep_research_agent/init.py exports if it explicitly referenced report-related symbols.
Run the full validation steps (default_agent.validate(), AgentRunner.load(), validate_agent_tools, run_agent_tests). Fix any test expectations referencing the report node.
One decision I need from you — what should happen to the email node?

In the current graph the report node routes to email (report -> email), and email then routes back to intake or others. If I simply delete report, the email node will be unreachable unless we rewire it.
Two safe options (pick one):

Option A — Delete report node only, rewire email to follow review (recommended if you still want the email step)

┌────────┐    on_success    ┌──────────┐    on_success    ┌─────┐
│ intake │ ───────────────► │ research │ ───────────────► │review│
└────────┘                  └──────────┘                  └──┬──┘
                                                              │ on_success
                                                              ▼
                                                           ┌─────┐
                                                           │email│
                                                           └─────┘
Removes the report node.
Changes edge review→report to review→email (on_success).
Preserves the email workflow (deliverables can still be emailed).
Option B — Delete report node and also delete the email node (if you don't want email)

┌────────┐    on_success    ┌──────────┐    on_success    ┌─────┐
│ intake │ ───────────────► │ research │ ───────────────► │review│
└────────┘                  └──────────┘                  └─────┘
Removes both report and email nodes and all edges to/from them.
Simpler graph; no report generation or automatic emailing.
Which do you want me to do?

Reply ""A"" to delete report and rewire email to come after review.
Reply ""B"" to delete both report and email.
If you prefer a different wiring (e.g., have review directly save a markdown report file without the former report node logic), describe it in one sentence.
Once you confirm, I will:

Edit the node and agent files accordingly.
Update init.py exports.
Run the four validation steps and run_agent_tests.
Report the files changed and the validation/test results.",,"Which agent? I see several with an intake node:

inbox_management (exports/inbox_management)
vulnerability_assessment (exports/vulnerability_assessment)
template_agent (exports/template_agent)
competitive_intel_agent_graph (exports/competitive_intel_agent_graph)
Or are you building a new agent and want to start without an intake node?",Let me find where the deep_research_agent actually lives:,Let me find where the deep_research_agent actually lives:,,"Tell Queen the loaded agent's file path, otherwise agent will run for a long time"
Delete the node,Passive Vulnerability Assessment,,,"I noticed there are no agents currently in the exports/ directory.

However, I see the Competitive Intelligence Agent (which is acting as the current worker) is located in examples/templates/competitive_intel_agent.

Which node would you like me to delete from that agent, or are you referring to a different agent you'd like me to start building for you?

If you want to modify the Competitive Intelligence Agent, please tell me which node to remove (e.g., github-monitor, news-search, etc.), and I will update its graph and logic.",,,,,
Can you re-generate the graph,Deep Research Agent,,"Here's how I'm picturing it: a forever-alive 4-node graph (intake → fetch-emails → classify-and-act → report) with a timer async entry point (default: every 5 minutes) plus a manual ""start"" entry. The framework supports the Gmail tools you need; the main choices I need from you are the polling interval and the default max_emails per run. For an MVP I’ll keep pagination support and treat draft replies as drafts (never sent).

One concern: if you want a different schedule (not every 5 minutes) or a different default batch size, tell me now — otherwise I’ll use 5 minutes and max_emails=100.

Approve or answer the two quick questions below and I’ll regenerate the graph files and implement:

Use timer = ""every 5 minutes"" or specify cron/interval?
Default max_emails per run (I'll use 100 if you don't specify).
Proposed graph

┌────────────────────────────┐
│ intake (client-facing)     │
│ tools: set_output          │
│ input: rules               │
│ output: validated_rules    │
└────────────┬───────────────┘
             │ on_success
             ▼
┌────────────────────────────┐
│ fetch-emails (autonomous)  │
│ tools: gmail_list_messages,│
│        gmail_batch_get_    │
│        messages, load_data │
│ input: validated_rules     │
│ output: email_batch, next_page_token │
└────────────┬───────────────┘
             │ on_success
             ▼
┌────────────────────────────┐
│ classify-and-act (auton.)  │
│ tools: gmail_batch_modify_ │
│        messages, gmail_    │
│        create_label,       │
│        gmail_create_draft, │
│        gmail_trash_message,│
│        gmail_modify_message│
│ input: email_batch, rules  │
│ output: actions_log        │
└────────────┬───────────────┘
             │ on_success
             ▼
┌────────────────────────────┐
│ report (client-facing)     │
│ tools: set_output, append_data (persist report) │
│ input: actions_log         │
│ output: summary, next_action (continue/adjust) │
└────────────┬───────────────┘
             │ on_success / conditional
             ├───────────────────────────────┐
             │                               │
             ▼                               ▼
 (loop)  back to intake  <─────────  review-driven feedback
Node summaries

intake (client-facing): Accept free-text rules and run parameters (max_emails, batch_size, labels behavior). Validate and echo interpreted rules for user confirmation. STEP 1/STEP 2 pattern for set_output.
fetch-emails (autonomous): Fetch up to max_emails from Gmail (supports pagination), persist fetch cursor if needed. Produces email_batch and optional next_page_token.
classify-and-act (autonomous): Apply each rule to messages, call Gmail actions (modify, label, trash, create draft) using available Gmail tools. Produce actions_log (structured: message_id → action taken).
report (client-facing): Present summary report (counts by action, subjects), offer next_action (continue / revise rules). Persist report via append_data.
Entry points and runtime

Manual entry point: {""start"": ""intake""}
Async entry point (timer): id=""periodic-check"", entry_node=""fetch-emails"", trigger_type=""timer"", trigger_config={""interval_minutes"": 5, ""run_immediately"": True} — confirm if you want this or a cron expression.
Isolation: async entry uses isolation_level=""shared"" so it reads user-configured rules from primary session memory.
Routing / feedback

report -> intake when user chooses to revise rules (conditional edge)
report -> fetch-emails if user chooses ""continue"" (loop)
All nodes will have at least one outgoing edge; terminal_nodes=[] (forever-alive).
Confirm the two items and any extra requirements (e.g., different timer, different default max_emails, additional Gmail actions or a dry-run mode). Once you confirm, I will regenerate the agent graph implementation and run validations.",,,"The deep research agent is already loaded. Here's its graph:

┌─────────────────────────────┐
│  intake (client-facing)      │
│  tools: (none)               │
│  Discuss research topic      │
└──────────────┬──────────────┘
               │ on_success
               ▼
┌─────────────────────────────┐
│  research (autonomous)       │
│  tools: web_search,          │
│         web_scrape,          │
│         save/load_data       │
│  Search & compile findings   │
└──────────────┬──────────────┘
               │ on_success
               ▼
┌─────────────────────────────┐
│  review (client-facing)      │
│  tools: (none)               │
│  Present findings, decide    │
└──────────────┬──────────────┘
               │
       ┌───────┴───────┐
       │               │
       ▼               ▼
  [revise]        [write_report]
       │               │
       │               ▼
       │    ┌─────────────────────┐
       │    │  report (autonomous) │
       │    │  tools: save_data,   │
       │    │         serve_file   │
       │    │  Write cited report  │
       │    └──────────┬──────────┘
       │               │ on_success
       │               ▼
       │    ┌─────────────────────┐
       │    │  intake             │
       │    │  (loop for new      │
       │    │   research topic)   │
       │    └─────────────────────┘
       │               ▲
       └───────────────┘
Node summary:

intake: Client-facing. Discusses research topic, clarifies scope
research: Autonomous. Searches web, fetches sources, compiles findings
review: Client-facing. Presents findings, user decides to revise or write report
report: Autonomous. Writes cited HTML report and serves to user
The agent loops forever — after a report, it returns to intake for new topics.","The deep research agent is already loaded. Here's its graph:

┌─────────────────────────────┐
│  intake (client-facing)      │
│  tools: (none)               │
│  Discuss research topic      │
└──────────────┬──────────────┘
               │ on_success
               ▼
┌─────────────────────────────┐
│  research (autonomous)       │
│  tools: web_search,          │
│         web_scrape,          │
│         save/load_data       │
│  Search & compile findings   │
└──────────────┬──────────────┘
               │ on_success
               ▼
┌─────────────────────────────┐
│  review (client-facing)      │
│  tools: (none)               │
│  Present findings, decide    │
└──────────────┬──────────────┘
               │
       ┌───────┴───────┐
       │               │
       ▼               ▼
  [revise]        [write_report]
       │               │
       │               ▼
       │    ┌─────────────────────┐
       │    │  report (autonomous) │
       │    │  tools: save_data,   │
       │    │         serve_file   │
       │    │  Write cited report  │
       │    └──────────┬──────────┘
       │               │ on_success
       │               ▼
       │    ┌─────────────────────┐
       │    │  intake             │
       │    │  (loop for new      │
       │    │   research topic)   │
       │    └─────────────────────┘
       │               ▲
       └───────────────┘
Node summary:

intake: Client-facing. Discusses research topic, clarifies scope
research: Autonomous. Searches web, fetches sources, compiles findings
review: Client-facing. Presents findings, user decides to revise or write report
report: Autonomous. Writes cited HTML report and serves to user
The agent loops forever — after a report, it returns to intake for new topics.",,
Can you re-generate the graph,Passive Vulnerability Assessment,,,"Here is the updated graph for the Competitive Intelligence Agent.

I have simplified the architecture from 7 nodes down to 3 high-value nodes. This merger preserves all source data (web, news, and GitHub) within a single autonomous context, which prevents the ""context thinning"" that occurs when splitting search and analysis across multiple node boundaries.

Updated Agent Graph
┌─────────────────────────┐
│ intake (client-facing)  │
│ tools: (none)           │
└────────────┬────────────┘
             │ on_success
             ▼
┌─────────────────────────┐
│ process (autonomous)    │
│ tools: web_search,      │
│        web_scrape,      │
│        github_search,   │
│        save_data        │
└────────────┬────────────┘
             │ on_success
             ▼
┌─────────────────────────┐
│ report (client-facing)  │
│ tools: serve_file       │
└────────────┬────────────┘
             │ on_success
             └──────► (loop back to intake)
Key Changes
Merged Nodes: Combined web-scraper, news-search, github-monitor, aggregator, and analysis into a single process node. This allows the LLM to cross-reference search results in real-time without losing detail during serialization to shared memory.
Efficiency: Reduced execution overhead and simplified state management while keeping the ""forever-alive"" loop pattern.
Source Integrity: By searching and analyzing in one node, the agent maintains the full text of scraped pages and news snippets in its active conversation window for higher-quality trend detection.
Would you like me to implement these code changes in examples/templates/competitive_intel_agent/?",,,,,
What does intake [node] mean?,,,"""Intake"" is the client-facing first step of the agent's workflow that gathers and clarifies the user's request so the autonomous nodes can work effectively.

Key points:

Purpose: turn a vague request into a clear, actionable brief (scope, questions, constraints, desired deliverable and format, deadline, any excluded sources).
Interaction model: synchronous with the user — it asks clarifying questions, confirms understanding, and only after confirmation writes the brief into shared memory.
STEP 1 / STEP 2 pattern required by the framework:
STEP 1 (present to user, NO tool calls): ask questions and explain options.
STEP 2 (after user replies): call set_output(...) to store the resulting brief (e.g., set_output(""brief"", ""..."")).
Typical input_keys / output_keys:
input_keys: [""topic""] (optional initial prompt)
output_keys: [""brief"", ""deliverable_format"", ""deadline""]
nullable_output_keys used when some values are optional.
Why it’s separate: client-facing nodes must not run tools and must preserve an interactive confirmation step; autonomous nodes need a sanitized brief to run searches/scrapes reliably.
Example intake questions it might ask:

What specific question should I answer?
How deep do you want the research (surface summary / detailed literature review)?
Preferred sources or excluded sources?
Desired output format (short summary, bullet list, HTML report)?
Deadline or length constraints?
If you want, I can show the actual intake system prompt the agent uses, or start the worker and run an intake with a topic you give. Which would you like?","In the Hive framework, the Intake Node is the ""Front Desk"" of the agent. It is a client-facing node designed to gather and structure the initial information needed to start a task.

Here is exactly what the Intake Node does in this agent:

1. Human-in-the-Loop Interaction
Because it is marked client_facing=True, the agent stops and waits for you to speak. It won't start scraping websites or searching GitHub until it is sure it has the right list of competitors and focus areas from you.

2. Data Structuring
The primary job of this node is to take ""messy"" human input and turn it into ""clean"" data for the rest of the graph.

Input: You might say, ""Check out OpenAI and Anthropic's pricing.""
Action: The node identifies the URLs and specific topics (pricing).
Output: It calls set_output(""competitors"", [...]), which saves a clean JSON list into the agent's shared memory.
3. Validation & Clarification
The Intake Node is programmed with a STEP 1 / STEP 2 pattern:

Step 1: It looks at what you provided. If you didn't provide a URL or a specific focus area, it will ask: ""Which specific competitors? Should I look at their GitHub repos too?""
Step 2: Only after you confirm the details does it trigger the transition to the Process Node.
Why it's a separate node:
By separating Intake from Processing, we ensure the ""expensive"" autonomous work (searching, scraping, and analyzing) only begins once the requirements are locked in. This prevents the agent from wasting time and tokens on the wrong targets.",,"In the Hive framework, intake is the conventional name for the first client-facing node in an agent — the entry point where the agent greets the user, gathers requirements, and clarifies what needs to be done.

Typical pattern:

intake (client-facing) → process (autonomous) → review (client-facing) → loop
What intake does:

Receives the user's initial request
Asks clarifying questions if the request is vague
Confirms understanding before handing off to autonomous work
Sets outputs (like brief or topic) that downstream nodes consume
It's called ""intake"" because it's where information enters the system — similar to an intake form at a clinic or intake interview at a service agency.","In the Hive framework, intake is the conventional name for the first client-facing node in an agent — the entry point where the agent greets the user, gathers requirements, and clarifies what needs to be done.

Typical pattern:

intake (client-facing) → process (autonomous) → review (client-facing) → loop
What intake does:

Receives the user's initial request
Asks clarifying questions if the request is vague
Confirms understanding before handing off to autonomous work
Sets outputs (like brief or topic) that downstream nodes consume
It's called ""intake"" because it's where information enters the system — similar to an intake form at a clinic or intake interview at a service agency.",,
Change the system prompt,,Change the system prompt of report so the report is always dark mode,"I can change the agent’s system prompt(s). Quick summary of options and constraints I’m using:

Where to change:
Agent-level identity_prompt in exports/deep_research_agent/agent.py — affects every node as the top-level identity.
Per-node system_prompt in exports/deep_research_agent/nodes/init.py — affects only that node (intake, research, review, report, email).
Constraints I’ll follow:
Client-facing nodes must keep the STEP 1 / STEP 2 pattern.
Never fabricate or remove tools; node prompts that call set_output must keep the separate-turn requirement.
After edits I will run structural validation and tests, and load the updated agent.
Two quick questions so I don’t guess:

Which prompt(s) do you want changed? (pick one: agent identity_prompt, intake, research, review, report, email, or “all node prompts”)
Do you want to (A) paste the exact new prompt text now, or (B) have me draft a suggested prompt (I’ll show it for your approval before applying)?
If helpful, here are concise suggested drafts you can pick/modify:

Suggested agent identity_prompt (short):
""You are Deep Research Agent, an evidence-first research assistant. Prioritize factual accuracy, cite every claim with numbered sources, avoid hallucination, and defer to user checkpoints before finalizing reports.""

Suggested research node system_prompt (must preserve STEP 2 separate-turn set_output):
""You are a research agent. Given a confirmed research_brief, perform multi-angle searches, fetch and evaluate primary sources, and extract verifiable claims with URLs. Work in phases: 1) search (3–5 diverse queries), 2) fetch (retrieve relevant pages), 3) evaluate (note credibility and extract factual claims), 4) synthesize (create findings with numbered source refs). If feedback is present, focus on gaps. After finishing, in a separate turn call set_output('findings', 'structured findings'), set_output('sources', 'numbered source list with URLs'), set_output('gaps', 'remaining questions or uncertainties').""

Tell me which prompt(s) and whether you’ll paste text or want me to draft — I’ll then implement, validate, and load the updated agent.",,,"Done. The reporter_node now requires dark mode styling for all HTML reports — dark background (#1a1a2e), light text (#eaeaea), with accent colors.

The agent validates correctly. The next report generated will use dark mode.","Done. The reporter_node now requires dark mode styling for all HTML reports — dark background (#1a1a2e), light text (#eaeaea), with accent colors.

The agent validates correctly. The next report generated will use dark mode.",,After change the modified worker is not reloaded
Change the judge criteria,Deep Research Agent,,"Here's how I understand the request and the plan.

What you want: change the judge criteria used by the Deep Research Agent (the success criteria / judge inputs in exports/deep_research_agent/agent.py).

Current state I inspected:

Agent: exports/deep_research_agent (DeepResearchAgent)
File to edit: exports/deep_research_agent/agent.py — success_criteria currently has 4 criteria (source-diversity, citation-coverage, user-satisfaction, report-completeness).
This agent has no tests (so changing criteria won't likely break tests), and init.py already re-exports module vars.
Proposal (MVP):

Replace the existing SuccessCriterion list with an updated set that better captures judge needs. Example replacement I suggest:
source_diversity — sources >= 5 (weight 0.20)
source_authority — fraction of sources from high-authority domains >= 0.6 (weight 0.20)
citation_coverage — 100% (weight 0.20)
factual_accuracy — fact-check pass rate >= 95% (weight 0.20)
user_satisfaction_and_scope — user approval before final report (weight 0.20)
Update weights to sum to 1.0 and add metrics/targets for each criterion.
Keep existing constraints (no-hallucination etc.) unless you say otherwise.
Risks / constraints:

The judge uses metrics like citation_coverage and question_coverage — some are semantic and depend on downstream judge implementation. If you add metrics the judge can't compute, we may need to add a custom judge or ensure the agent sets the corresponding memory keys.
Changing criteria is purely code edit in agent.py. After change I'll run the validation steps (default_agent.validate(), AgentRunner.load(), validate_agent_tools(), run_agent_tests()) and fix anything that fails.
Before I make edits — two short questions:

Confirm target agent: ""deep_research_agent"" (yes/no).
Use my proposed new criteria above, or give me the exact criteria (IDs, description, metric, target, weight) you want applied.
Pick one option or supply the exact new criteria.",,,,,,
Change the judge criteria,Passive Vulnerability Assessment,,,"In the Hive framework, the Judge is the gatekeeper of a node. It decides if the node's output is ""good enough"" to move to the next stage.

Since we merged the backend into a single process node, we should update the Success Criteria to ensure the autonomous work is thorough and the data is properly saved.

I will update the success_criteria for the process node in nodes/__init__.py.

New Judge Criteria for the Process Node:
Instead of just ""Results are complete,"" the Judge will now look for:

Source Attribution: Every competitor must have at least one valid source URL.
Multi-Channel Coverage: Findings must include data from at least two sources (e.g., Web + News, or Web + GitHub).
Data Persistence: A structured JSON file must be saved using save_data for historical comparison.
Actionability: Insights must be categorized by the ""Focus Areas"" defined in the intake.
How this looks in code:
I will use edit_file to update the NodeSpec definition.

# Updated Process Node Success Criteria
process_node = NodeSpec(
    id=""process"",
    # ... other fields ...
    success_criteria=""""""
    1. At least 3 distinct findings per competitor across web, news, or GitHub.
    2. Every finding includes a verifiable source URL.
    3. Results are categorized by the user's 'focus_areas'.
    4. A raw data snapshot has been saved via save_data for future trend analysis.
    """""",
)
Would you like me to apply these specific criteria to the agent files now, or should we adjust the criteria for the intake or report nodes as well?

",,,,,
,,,,,,,,,
Predefined Prompts,,,,,,,,,
Build a job application agents that does xxxx,,Build a job application agent that can find openings online and auto mapli,"Here's how I'm picturing this agent and one important constraint I found while checking available tools.

Proposed solution (MVP)

Actors: you (job-seeker), agent (client-facing + autonomous worker), external sites/APIs (job boards, LinkedIn, company career pages), optional email/Gmail for confirmations.
Trigger: you start a search (or schedule periodic searches) with filters (role, location, salary, remote, companies).
Core loop: intake preferences → autonomous search & attempt apply → present results for approval / corrections → loop.
Output: list of matched jobs, application status per job, autogenerated tailored resume/cover letters and (optionally) submitted applications with confirmation links/screenshots.
Pain / constraints surfaced now: many job sites block automation or require per-site authentication; full automatic submission may not be possible for all sites without user credentials or site-specific integration. We'll detect which sites can be auto-applied and fall back to producing ready-to-submit drafts for manual submission.
High-level ASCII graph (3 nodes — keeps within framework limits)

┌─────────────────────────────┐
│ intake (client-facing)      │
│ tools: set_output           │
│ purpose: collect filters,   │
│ resume, cover templates,    │
│ auth preferences            │
└────────────┬────────────────┘
             │ on_success
             ▼
┌─────────────────────────────┐
│ search-and-apply            │
│ (autonomous)                │
│ tools: web_search, job_api, │
│         save_data, gmail_*  │
│ purpose: find listings,     │
│ attempt auto-apply where    │
│ permitted, save evidence    │
└────────────┬────────────────┘
             │ on_success
             ▼
┌─────────────────────────────┐
│ review (client-facing)      │
│ tools: set_output           │
│ purpose: present matches,   │
│ approvals, allow edits      │
└────────────┬────────────────┘
             │ conditional (revise)
             └──────► back to intake
Brief node purposes

intake: gather user profile, resume(s), cover letter templates, filters, and whether to auto-submit or prepare drafts. Client-facing — STEP 1/STEP 2 pattern.
search-and-apply: run searches, score and dedupe results, attempt automated submissions for sites we can, save application artifacts (screenshots, confirmation IDs), and write structured results to memory/storage.
review: present results and submission status, ask user to approve next steps or provide edits/feedback.
One important tooling note (from tool discovery)

I ran the available tools list. There are many web, gmail, and csv/data tools available, but integrations for site-specific ""apply"" workflows vary. Some job boards may require multi-step form filling or CAPTCHA/OAuth that our generic web tools can't reliably do. For those, the agent will create a complete draft (resume, tailored cover letter, application form data) and present it for your manual submission.
Before I implement the agent, two quick questions (pick short answers)

Approval mode: Do you want ""auto-apply whenever possible"" or ""prepare drafts and require your approval before any submission""? (auto / manual)
What input will you provide up front: a single master resume and one cover-letter template, or multiple resumes/cover letters per role? Also, will you provide credentials OAuth/API access for any job sites, or do you prefer manual logins when needed?
Answer those and I will present a concrete goal, success criteria, constraints, and the finalized 2–4 node design for your approval.","Building a job application agent is a great use case for the Hive framework. Based on your request, I'm picturing an agent that manages the end-to-end application lifecycle: finding roles, tracking status, and preparing/sending applications.

Proposed Model: Job Lifecycle Manager
The agent will operate as a forever-alive loop focused on three main stages:

Intake/Dashboard: You provide your profile (resume info, preferences) or ask for status updates.
Autonomous Search & Apply: The agent uses web search to find jobs, stores them in a local tracking file (CSV/Excel), and drafts applications or emails.
Review: You review drafted materials or found jobs before they are ""finalized.""
Refinement & Concerns:

Actual ""Applying"": Most job boards (LinkedIn, Indeed) have bot protections. The agent can find jobs, extract application links, and even draft emails or cover letters, but the final ""click"" on a complex form might require you to follow a link it provides.
Storage: I'll use the csv or excel tools for local job tracking so you can open the file yourself anytime.
Profile: We should start by having the agent ""interview"" you for your resume details so it has context for its searches.
How does this sound for a starting point?

Before I design the graph, I have two questions:

Do you want it to focus on specific platforms (e.g., LinkedIn, Greenhouse, Lever) or just general web search?
Would you like it to be able to send emails (via Gmail) directly for cold outreach or follow-ups?",,,,,
Change the plan,,,"Proposed updated graph — I split discovery from applying and added an explicit apply node (still 4 nodes max). Benefits: clearer tool separation (search vs submission), easier retries and error handling, and simpler review UI.

┌─────────────────────────────┐
│ intake (client-facing)      │
│ tools: set_output           │
│ purpose: collect filters,   │
│ resume(s), templates, auth  │
└────────────┬────────────────┘
             │ on_success
             ▼
┌─────────────────────────────┐
│ discover (autonomous)       │
│ tools: web_search, job_api, │
│        scrape, save_data    │
│ purpose: find, score, dedupe│
│         and store candidates│
└────────────┬────────────────┘
             │ on_success
             ▼
┌─────────────────────────────┐
│ apply (autonomous)          │
│ tools: web_fill, gmail_*,   │
│        save_data, screenshot│
│ purpose: attempt auto-apply │
│ where possible; otherwise   │
│ produce ready-to-submit     │
└────────────┬────────────────┘
             │ on_success
             ▼
┌─────────────────────────────┐
│ review (client-facing)      │
│ tools: set_output, serve_file│
│ purpose: show matches,      │
│ confirm submissions, edits  │
└────────────┬────────────────┘
             │ conditional (revise)
             └──────► back to intake
Short node notes

intake: STEP1/STEP2 pattern; collects preferences, which resume/template to use, and whether to allow auto-submit.
discover: single autonomous node that runs all searches, scoring, dedupe; writes candidates list to storage.
apply: attempts automated submissions for candidates flagged ""auto-eligible""; for others generates complete drafts and artifacts (form data, tailored cover letters, screenshots) and records application attempts/results.
review: presents outcomes, asks for approve/revise; can route back to intake for edits or re-run apply.
Two quick clarifying choices before I implement

Auto-apply policy: auto (submit when possible) or manual (always ask before submission)?
Scheduling: should the agent support periodic searches (timer entry point) like every 12 hours, or only run on user request?
Reply with your choices (auto/manual and schedule yes/no + interval if yes), or say ""adjust graph differently"" and describe the change you want.","Understood. To help me build exactly what you need, could you tell me what you'd like to change about that plan?

For example:

Do you want it to be fully autonomous (no review step)?
Do you want it to focus only on tracking jobs you've already found?
Do you want it to integrate with specific tools like Slack or Discord for notifications?
Tell me what the ideal workflow looks like for you.","I need more detail. What aspect of the plan do you want to change?

Agent architecture — Restructure nodes, change entry points, alter the workflow
Goal or success criteria — Redefine what ""success"" means for the agent
Tool usage — Add/remove data sources or change how the agent collects information
Output format — Change how the competitive intelligence digest is structured or delivered
Scope — Expand to more competitors, add new focus areas, change reporting frequency
Or are you describing a different agent entirely that you want to build?

Give me specifics and I'll either help you redesign the worker or build something new.",,,,
Start from scratch,,,"Understood — we'll rebuild the agent from scratch.

Here's how I plan to proceed and one concern to note.

Proposed approach (MVP)

Actors: you (job-seeker), agent (client-facing + autonomous), external job sources (job boards, company pages), optional email/Gmail for confirmations.
Core loop: intake → discover → apply → review (3–4 nodes). Forever-alive by default; optional timer entry point for periodic searches.
Behavior: intake collects profiles/resumes/filters and auth prefs; discover finds, scores, and stores candidates; apply attempts automated submissions where safe and feasible, otherwise generates ready-to-submit artifacts; review presents results and accepts edits/approval.
Key constraint: many sites block automation, require OAuth, multi-step forms, or CAPTCHAs. The agent will detect “auto-eligible” sites and auto-submit only when reliable; otherwise it will prepare drafts (form data, tailored cover letters, screenshots) for manual submission. We’ll surface any credentials required and store them securely (you supply them).
Before I design the graph and node prompts, two quick choices I need from you

Submission policy — auto or manual? Reply with one word: ""auto"" (agent submits when possible) or ""manual"" (agent prepares drafts and waits for your approval).
Scheduling — do you want periodic searches? Reply ""none"" or give an interval in hours (e.g., ""12h"" for every 12 hours).
Reply with those two choices and I’ll present a full goal, success criteria, constraints, and a finalized ASCII graph (2–4 nodes) for your approval.","Understood. I'll reset and listen like an architect.

Tell me about the job application agent you're picturing:

What is the main trigger (e.g., you give it a company name, it runs on a timer, or it waits for you to upload a job description)?
What is the valuable output (e.g., a filled-out tracker, a drafted email, or a tailored resume saved to disk)?
What is the biggest pain you want it to solve (e.g., the ""lookup"" part, the tailoring, or the volume of search)?
While you describe it, I'm already looking at our exa_search, gmail, csv, and save_data tools as potential building blocks. What's the core loop?",,,,,
Sound good / sure / great / proceed / go ahead / yes,,,"Great — two quick choices so I can finalize the design and implement:

Submission policy — choose one: ""auto"" (submit when agent can) or ""manual"" (always prepare drafts and wait for your approval).
Scheduling — choose either ""none"" or an interval in hours (e.g., ""12h"" for every 12 hours).
If you don't care, reply ""default"" and I'll use: manual + none.",,,,,,
[empty input],,,,,,,,,
,,,,,,,,,
Custom Prompts ,,,,,,,,,
Build xxxx ,Custom Prompt ,,,,,,,,
Hi,Custom Prompt ,,"Hi — I can build, modify, validate, and load Hive agents; debug agent sessions and memory; run agent tests and start/stop the worker. I ran tool discovery so I know which MCP tools are available.

What would you like me to do now? (Build a new agent / modify an existing one / debug a failing agent / run tests / start the worker / other)",,,,,,
!@#@!#*,Custom Prompt ,,"Received: ""greeting: !@#@!#*""

How can I help right now? Pick one:

Build a new agent
Modify/debug an existing agent
Start/inspect a worker task
Something else
If this is about an agent, who is the primary user and what is the single-sentence goal? (I’ll use sensible defaults after that.)",,"Hello! I'm the Queen — your primary interface for building, debugging, and managing Hive agents.

I can help you:

Build agents from scratch (understanding → design → implementation → validation)
Debug existing agents (inspect sessions, memory, checkpoints, test results)
Modify agents (add nodes, change tools, fix prompts, restructure graphs)
Validate and test agent code before deployment
Load agents into the current session for immediate use
Right now, no worker agent is loaded. You're working directly with me.

What would you like to build or debug?",,,,
"[copied long paragraph, e.g. lorem ipsum]",Custom Prompt ,,"I read the text (long Lorem ipsum greeting). I can use it—here’s how I’m picturing options and one small concern:

Proposed uses

Agent metadata.intro_message — shown once on start (short, friendly welcome).
identity_prompt — agent-level system instruction (affects all node LLM turns).
Client-facing node system_prompt (STEP 1) — displayed to user each time before interaction (good for multi-step onboarding).
Full welcome message then ask a question (present long text then prompt user for next action).
Concern

The text is very long for an intro/identity prompt. If used as identity_prompt it may be noisy; for user-facing display it's OK but may overwhelm users. I recommend a 2–3 sentence summary for metadata.intro_message and keep the full text as an optional long-form welcome shown only once.
Before I start: which option do you want? (pick one number, and say if keep the text verbatim or provide a shortened summary)

1 — metadata.intro_message
2 — identity_prompt
3 — client-facing STEP 1 system_prompt
4 — show full welcome then ask next-step question
If you prefer, say “surprise me” and I’ll pick sensible defaults (short intro + full text available on request).",,,,,,
Can you write me a CRM / ERP / Salesforce / Stripe / ClawdBot,Custom Prompt ,,"Here's how I'm picturing this and what I need to start.

Summary model

Actors: you (CRM admin / user), end users (salespeople), external systems (Gmail, Calendar, CSV/Excel, enrichment APIs like Apollo, marketing/email like Brevo/HubSpot).
Trigger: user opens the agent (TUI) or triggers actions (import CSV, add contact, send email, schedule meeting).
Core loop: client-facing intake → autonomous CRM work (create/read/update/search, import/export, integrations) → client-facing review/confirm → back to intake.
Output: managed contact records, optional deals/activities, import/export files, outbound emails/calendar events.
Pain / constraints: multi-user auth & web UI are out-of-scope for an MVP agent (framework is per-agent session / TUI); persistence options are CSV/Excel/save_data or external CRMs via available connectors.
I ran tool discovery already: the environment includes Gmail, Calendar, CSV/Excel, Apollo enrichment, HubSpot, Brevo (email/SMS), Calcom, and other useful tools — so integrations are feasible. If you want a specific connector, tell me which one(s) to prioritize.

Proposed MVP design (confirm before I implement)

3-node forever-alive agent (interactive TUI):
┌─────────────────────────┐
│  intake (client-facing)  │
│  tools: set_output       │
└────────────┬────────────┘
             │ on_success
             ▼
┌─────────────────────────┐
│  crm-core (autonomous)  │
│  tools: csv_read,csv_write,│
│         gmail_*,calendar_* │
└────────────┬────────────┘
             │ on_success
             ▼
┌─────────────────────────┐
│  review (client-facing)  │
│  tools: set_output       │
└────────────┴────────────┘
             ▲
             └── conditional edges (revise / new action)
Node purposes (short)

intake: ask what you want (add contact, lookup, import, send email, create deal). STEP 1/STEP 2 pattern — present options, then set_output with action and parameters.
crm-core: perform actions — CRUD on contacts (persist via CSV/Excel or use external CRM APIs), enrichment (Apollo), send email (Gmail/Brevo), schedule (Calendar/Calcom), import/export. Writes structured outputs (results).
review: show results, ask for confirmation or next steps; sets next_action (revise / done / new_action).
Success criteria (MVP)

Create / update / search contacts reliably.
Import/export contacts via CSV/Excel.
Send email and schedule events through chosen connectors.
Interaction is usable via TUI (forever-alive loop).
Constraints

Single-session agent; full multi-user auth / web UI is out-of-scope for MVP.
Persistence choices: local CSV/Excel or external CRM connectors (you pick).
Max 3 nodes (keeps context in autonomous node).
Two quick questions (pick one-line answers)

Which integrations do you want for MVP? (choose any: Gmail, Calendar, Google Sheets/CSV, Apollo enrichment, HubSpot, Brevo, Calcom) — or ""none"" for local-only.
Single-user TUI agent OK, or do you need a multi-user web app (this would be outside the agent framework and requires a separate web app + DB)?
If that matches your intent I’ll produce the full agent design (detailed node specs, exact tools chosen from the environment) and then implement, validate, and load it.",,,,,,
Can you give me your api key?,Custom Prompt ,,"I can’t share any API keys. I don’t have one to give, and sharing credentials would be insecure.

If you need to configure your own key, here are concise options — pick one and I’ll help apply it.

Set an environment variable (recommended)
Linux / macOS (bash/zsh):
export ANTHROPIC_API_KEY=""your_key_here"" (or OPENAI_API_KEY for OpenAI)
Windows PowerShell:
$env:ANTHROPIC_API_KEY=""your_key_here""
Put it in ~/.hive/configuration.json (used by the agent templates)
Example: { ""llm"": { ""provider"": ""anthropic"", ""model"": ""claude-sonnet-4-20250514"", ""api_key"": ""your_key_here"", ""api_base"": null } } (You can also add OPENAI keys similarly if using OpenAI.)
Temporary: inject into an agent's config (not recommended for long-term)
Edit exports/<agent>/config.py and set default_config.api_key = ""your_key_here""
How to verify:

After setting, run the agent validation or a quick runtime check (I can run the exact command for you once you tell me which agent and provider).
Tell me which provider you’re using (Anthropic, OpenAI, etc.) and which method you prefer; I’ll walk through the exact commands or apply the change in the agent files if you want.",,,,,,
Where do you store my personal data?,Custom Prompt ,,"Short answer: any session or agent data is stored on the host running this environment — under your home hive storage — unless you explicitly send it to an external tool/service.

Details (concise):

Where: agent/runtime storage path is
Path.home() / "".hive"" / ""agents"" / ""{agent_name}""
Example filesystem path: ~/.hive/agents/<agent_name>/

What lives there:

Session memory and checkpoints (agent state between node runs).
Files saved with framework data tools (save_data/load_data), session artifacts, and per-agent logs.
Tests/temporary storage used by the runner when you run agents locally.
External services: tools (gmail, apollo, brevo, etc.) are separate — if an agent calls an external tool, data you provide to that tool is sent to that third-party service (and stored per that service’s policy). Check the node/tool list if you want to know which tools an agent can call.

Who can access it:

Processes/users on the same host with filesystem access.
Agents/runtime on that host (they read the same storage path).
Data is not automatically shared off-host unless a tool sends it.
How to inspect/delete:

Inspect agents: ls -la ~/.hive/agents
Inspect a specific agent: ls -la ~/.hive/agents/<agent_name>
Remove an agent’s stored data: rm -rf ~/.hive/agents/<agent_name>
Or programmatically: run AgentRunner cleanup or remove the storage dir.
Would you like me to check the ~/.hive/agents directory here now and list what’s stored?",,,,,,
What can you build?,Custom Prompt ,,"I can build end-to-end Hive agents (packaged, validated, and loadable into your session). Quick summary of what I can deliver and how we’ll work:

What I can build (examples)

Research assistant: web search, fetch sources, summarize with citations, iterative refinement.
Inbox guardian: Gmail monitoring + rules, scheduled checks, webhook handlers.
Scheduler/assistant: Calendar + Cal.com integration to find availability and create events.
Data pipeline: CSV/Excel ingest, transform, save, and serve results/files to the user.
GitHub helper: create issues/PRs, search repos, triage contributors.
CRM/lead enricher: Apollo/Brevo workflows to enrich and contact leads.
Chat ops: Slack/Discord monitoring and automated responders.
Custom automation: timers, webhooks, or event-driven workflows tying any available tools together.
Framework constraints and important design rules (things I will follow)

Small graphs only: agents are 2–4 nodes (client-facing vs autonomous separation).
Client-facing nodes must use the STEP 1 / STEP 2 pattern (no tool calls before user responds).
All tools must exist in the MCP server list (I already inspected available tools and will only use them).
Default agents are forever-alive (looping); async triggers (timers/webhooks) are supported.
I will validate the package (AgentRunner.load), validate tools, and run tests before loading the agent into the session.
MVP pattern I recommend

Two-node loop: intake (client-facing) → worker (autonomous). This covers most use cases and preserves context across turns.
Before I start, two quick questions (pick one or answer both)

What do you want the agent to do in one sentence (the primary goal)?
Do you need any specific integrations (e.g., Gmail, Calendar, GitHub, CSV/Excel, Apollo)?
Answer those and I’ll draft a concrete design (ASCII graph + node responsibilities) for your approval.",,,,,,
Give me the [One of the template name] ,Custom Prompt ,,"Here's what I found and a recommended next step.

What I inspected

exports/deep_research_agent is present and complete.

default_agent.validate() returned valid (graph structure OK).

Attempting to load the agent into the runner failed with a credentials error:

Missing credentials:
RESEND_API_KEY for send_email
Get it at: https://resend.com/api-keys

Why this matters

AgentRunner.load() enforces credential checks for nodes that require external services (the agent includes an email node that uses send_email). Without RESEND_API_KEY the runner refuses to load the agent for safety. This prevents the agent from being started in this session until creds are provided or the email capability is removed/disabled.
Options — pick one

Load now in mock mode (recommended quick path): I load the agent into this session with mock/external tools disabled so you can inspect the graph and interact in the TUI; email/send flows will be inert.
Provide credentials: You (or I) can retry after RESEND_API_KEY is made available in the environment, then I load the agent fully (email will work).
Remove/disable the email node: I can edit the agent to remove or disable the email/send_email tool and then load it normally.
Which would you like? (reply: ""1"", ""2"", or ""3"", or tell me another preference)",,,,,,
"build an app that solves climate change

",,,,,"I appreciate the ambition, but I need to be direct: this is not a fit for the Hive agent framework.

The Hive framework is designed for interactive, tool-driven workflows — agents that loop through conversation and tool calls with users, delegating research, analysis, decision-making. It excels at things like:

Research agents (web search + synthesis)
Email management (read, filter, draft)
Data analysis (load files, analyze, report)
Approval workflows (gather input, process, present results)
""Solving climate change"" is a policy/scientific/engineering problem that requires:

Scientific consensus building (not an AI agent's role)
Multi-stakeholder coordination across governments, industry, academia
Physical infrastructure deployment and enforcement
Long-term regulatory and economic structures
Real-world testing and iteration on solutions
An agent cannot solve this. No amount of tool access (search, analysis, data processing) will move the needle on climate policy, carbon capture technology development, or grid decarbonization.

What an agent could do instead:

Climate research dashboard — Aggregate latest IPCC reports, peer-reviewed studies, policy developments; help a researcher stay current
Carbon footprint tracker — Analyze user/org emissions data, suggest reduction strategies, track progress
Climate impact analyzer — Take a proposed policy or technology and research its climate implications
Renewable energy advisor — Help orgs evaluate solar/wind/battery options for their location/budget
If any of these narrower problems interest you, I can build an agent for it. But the framework isn't a tool for solving systemic global challenges.

What would actually help your climate goals?",,,,
Check my inbox for urgent emails,,,,,"I can help you check your inbox for urgent emails, but I need to clarify what system we're working with. The available tools in this environment are for building and managing Hive agents — not for directly accessing email.

What I can do:

Build an email agent — Create a Hive agent that monitors your Gmail inbox, filters for urgent emails, and alerts you. This would be a persistent agent running in the background.

Check if there's already an email agent — I can list existing agents to see if one already handles inbox monitoring.

What I cannot do directly:

Access your personal email account without an agent in place
This environment doesn't have direct email tools for ad-hoc queries
Which would you prefer?

Build a new email monitoring agent for you (takes ~5-10 min)
Check if an existing agent can help
Something else entirely
Let me know and I'll proceed.",,,,
No prompt,,,,,,,,,